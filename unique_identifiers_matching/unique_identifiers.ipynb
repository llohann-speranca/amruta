{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def test_unique_join(files_df: pl.DataFrame, database_df: pl.DataFrame):\n",
    "    return files_df.join(database_df, on=[\"date\"]).group_by(\"file_stem\").len().sort(\"len\", descending=True)\n",
    "\n",
    "\n",
    "files_dir = Path(\"2200 Talks per directory\")\n",
    "database_df = pl.read_excel(\n",
    "    \"Progress - for Lohan.xlsm.xlsx\"\n",
    ")\n",
    "database_df = database_df.drop_nulls(\"Date\").with_columns(pl.col(\"Date\").dt.strftime(\"%Y-%m-%d\").alias(\"date\")).filter(pl.col(\"Summary\").str.to_lowercase().str.contains_any([\"poem\", \"page\", \"extract\",\"bhajans\", \"concert\", \"evening program\", \"letter\", \"quote\", \"arriv\"]).not_()).with_columns(pl.col(\"Summary\").str.slice(11).alias(\"Summary\"))\n",
    "\n",
    "database_with_date_df = database_df.filter(pl.col(\"date\")!= \"1970-01-01\")\n",
    "\n",
    "files = list(files_dir.rglob(\"*.docx\"))\n",
    "files_df = pl.DataFrame({\"file_stem\": [f.stem for f in files]})\n",
    "\n",
    "files_df = files_df.with_columns(pl.col(\"file_stem\").str.extract(r\"^(\\d{4}-\\d{2}-\\d{2})\").alias(\"date\"))\n",
    "files_df = files_df.with_columns(pl.col(\"file_stem\").str.replace(r\"^(\\d{4}-\\d{2}-\\d{2})\", \"\").str.strip_chars(\" â€“-\").alias(\"Title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate files without date\n",
    "files_with_date_df = files_df.filter(pl.col(\"date\")!= \"1970-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files without date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_without_date_df = files_df.filter(pl.col(\"date\")== \"1970-01-01\")\n",
    "# files_without_date_df.write_excel(\"to_be_matched_manually.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Match files and database with unique dates\n",
    "Almost sure matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_unique_date_df = files_with_date_df.filter(pl.col(\"date\").len().over(\"date\") == 1)\n",
    "database_with_unique_date_df = database_with_date_df.filter(pl.col(\"date\").len().over(\"date\") == 1)\n",
    "\n",
    "test_unique_join(files_with_unique_date_df, database_with_unique_date_df).item(0, \"len\")==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = matched_step_1_df = files_with_unique_date_df.join(database_with_unique_date_df, on=\"date\", suffix=\"_amruta\").with_columns(pl.lit(\"1_unique_dates\").alias(\"match_step\"))\n",
    "# joined_df.write_excel(\"almost_sure_matches.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Match by date and city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matched_in_step_1_df = files_with_date_df.filter(pl.col(\"file_stem\").is_in(matched_step_1_df['file_stem']).not_())\n",
    "\n",
    "print(f\"Nothing wrong happened so far? {len(non_matched_in_step_1_df) + len(matched_step_1_df) == len(files_with_date_df)}\")\n",
    "\n",
    "print(f\"Yet to be matched: {non_matched_in_step_1_df['file_stem'].n_unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matched_in_unique_date_partial_match_df = non_matched_in_step_1_df.join(database_with_date_df, on=\"date\", suffix=\"_amruta\")\n",
    "\n",
    "matched_step_2_df = non_matched_in_unique_date_partial_match_df.filter(pl.col(\"file_stem\").str.contains(pl.col(\"City\"))).filter(pl.col(\"file_stem\").len().over(\"file_stem\")==1, pl.col(\"Amruta\").len().over(\"Amruta\") ==1).with_columns(pl.lit(\"2_date_and_city\").alias(\"match_step\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = pl.concat([all_matches, matched_step_2_df], how=\"diagonal\")\n",
    "\n",
    "database_minus_step_2 = database_with_date_df.filter(pl.col(\"Amruta\").is_in(all_matches['Amruta']).not_())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Match data and [:5] string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matched_step_2_df = non_matched_in_step_1_df.filter(pl.col(\"file_stem\").is_in(matched_step_2_df['file_stem']).not_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_step_3_df = non_matched_step_2_df.join(database_minus_step_2, on=\"date\", suffix=\"_amruta\").filter(pl.col(\"Title\").str.replace_all(\"[^A-Za-z]\", \"\").str.contains(pl.col(\"Title_amruta\").str.replace_all(\"[^A-Za-z]\", \"\").str.slice(0,5))).filter(pl.col(\"file_stem\").len().over(\"file_stem\")==1).with_columns(pl.lit(\"3_date_and_string\").alias(\"match_step\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = pl.concat([all_matches, matched_step_3_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_minus_step_3 = database_minus_step_2.filter(pl.col(\"Amruta\").is_in(all_matches['Amruta']).not_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: More unique dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matched_step_3_df = non_matched_step_2_df.filter(pl.col(\"file_stem\").is_in(matched_step_3_df['file_stem']).not_())\n",
    "print(len(non_matched_step_3_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_step_4_df = non_matched_step_3_df.join(database_minus_step_3, on=\"date\", suffix=\"_amruta\").filter(pl.col(\"file_stem\").len().over(\"file_stem\")==1).with_columns(pl.lit(\"4_more_unique_dates\").alias(\"match_step\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = pl.concat([all_matches, matched_step_4_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_minus_step_4 = database_minus_step_3.filter(pl.col(\"Amruta\").is_in(all_matches['Amruta']).not_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Tf-IDf similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matched_step_4_df = non_matched_step_3_df.filter(pl.col(\"file_stem\").is_in(matched_step_4_df['file_stem']).not_())\n",
    "\n",
    "print(len(non_matched_step_4_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Get the titles from the non-matched dataframe\n",
    "titles = non_matched_step_4_df['Title'].to_list()\n",
    "\n",
    "# Fit and transform the titles to TF-IDF vectors\n",
    "titles_vectorized = tfidf_vectorizer.fit_transform(titles)\n",
    "\n",
    "# Print the shape of the resulting matrix\n",
    "print(f\"TF-IDF matrix shape: {titles_vectorized.shape}\")\n",
    "\n",
    "# If you want to see the feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Number of features (unique words): {len(feature_names)}\")\n",
    "print(f\"Sample features: {feature_names[:10]}\")\n",
    "\n",
    "# Transform the database titles to TF-IDF vectors\n",
    "database_vectorized = tfidf_vectorizer.transform(database_minus_step_4['Title'])\n",
    "# Calculate similarity matrix using dot product between TF-IDF vectors\n",
    "\n",
    "\n",
    "# Matrix multiplication of tfidf_matrix with the transpose of database_vectorized\n",
    "# This gives us a similarity score between each title in non_matched_step_4_df and each title in database_minus_step_4\n",
    "similarity_matrix = titles_vectorized.dot(database_vectorized.transpose())\n",
    "\n",
    "# Print the shape of the similarity matrix\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "# The shape should be (n_non_matched_titles, n_database_titles)\n",
    "\n",
    "# Optional: Convert to dense array for easier inspection of a sample\n",
    "similarity_sample = similarity_matrix[:5, :5].toarray()\n",
    "print(\"Sample of similarity matrix (first 5x5):\")\n",
    "print(similarity_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_step_5_df = non_matched_step_4_df.with_row_index().join(database_minus_step_4.with_row_index(), on=\"date\", suffix=\"_amruta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_score(index, index_amruta, similarity_matrix=similarity_matrix):\n",
    "    return similarity_matrix[index, index_amruta]\n",
    "\n",
    "joined_step_5_df = joined_step_5_df.with_columns(pl.struct(\"index\", \"index_amruta\").map_elements(lambda x: get_similarity_score(**x), return_dtype=pl.Float64).alias(\"similarity_score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_step_5_df.group_by(\"file_stem\").agg(pl.col(\"similarity_score\").max().alias(\"max_score\"), pl.col(\"similarity_score\").eq(0.).not_().sum().alias(\"n_non_zero_scores\")).sort(\"n_non_zero_scores\", descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Get the titles from the non-matched dataframe\n",
    "titles = files_df['Title'].to_list()\n",
    "amruta_titles = database_df['Title'].to_list()\n",
    "summaries = database_df['Summary'].to_list()\n",
    "# Fit and transform the titles to TF-IDF vectors\n",
    "tfidf_vectorizer.fit(titles+amruta_titles+summaries)\n",
    "\n",
    "titles_vectorized = tfidf_vectorizer.transform(titles)\n",
    "amruta_titles_vectorized = tfidf_vectorizer.transform(amruta_titles)\n",
    "summary_vectorized = tfidf_vectorizer.transform(summaries)\n",
    "# If you want to see the feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Number of features (unique words): {len(feature_names)}\")\n",
    "print(f\"Sample features: {feature_names[:10]}\")\n",
    "\n",
    "\n",
    "# Matrix multiplication of tfidf_matrix with the transpose of database_vectorized\n",
    "# This gives us a similarity score between each title in non_matched_step_4_df and each title in database_minus_step_4\n",
    "similarity_matrix = titles_vectorized.dot(amruta_titles_vectorized.transpose())\n",
    "similarity_matrix_summary = titles_vectorized.dot(summary_vectorized.transpose())\n",
    "# Print the shape of the similarity matrix\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "# The shape should be (n_non_matched_titles, n_database_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Get the titles from the dataframes\n",
    "titles = files_df['Title'].to_list()\n",
    "amruta_titles = database_df['Title'].to_list()\n",
    "summaries = database_df['Summary'].to_list()\n",
    "\n",
    "# Tokenize the documents\n",
    "tokenized_titles = [doc.lower().split() for doc in titles]\n",
    "tokenized_amruta_titles = [doc.lower().split() for doc in amruta_titles]\n",
    "tokenized_summaries = [doc.lower().split() for doc in summaries]\n",
    "\n",
    "# Create BM25 models\n",
    "bm25_titles = BM25Okapi(tokenized_amruta_titles)\n",
    "bm25_summaries = BM25Okapi(tokenized_summaries)\n",
    "\n",
    "# Create similarity matrices\n",
    "similarity_matrix = np.zeros((len(titles), len(amruta_titles)))\n",
    "similarity_matrix_summary = np.zeros((len(titles), len(summaries)))\n",
    "\n",
    "# Calculate BM25 scores for each query (title) against all documents\n",
    "for i, query in enumerate(tokenized_titles):\n",
    "    title_scores = bm25_titles.get_scores(query)\n",
    "    summary_scores = bm25_summaries.get_scores(query)\n",
    "    similarity_matrix[i] = title_scores\n",
    "    similarity_matrix_summary[i] = summary_scores\n",
    "\n",
    "# Convert to sparse matrices for compatibility with existing code\n",
    "similarity_matrix = csr_matrix(similarity_matrix)\n",
    "similarity_matrix_summary = csr_matrix(similarity_matrix_summary)\n",
    "\n",
    "# Get unique terms for feature names (similar to TF-IDF feature names)\n",
    "all_tokens = set()\n",
    "for doc in tokenized_titles + tokenized_amruta_titles + tokenized_summaries:\n",
    "    all_tokens.update(doc)\n",
    "feature_names = np.array(list(all_tokens))\n",
    "\n",
    "print(f\"Number of features (unique words): {len(feature_names)}\")\n",
    "print(f\"Sample features: {feature_names[:10]}\")\n",
    "\n",
    "# Print the shape of the similarity matrix\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "# The shape should be (n_non_matched_titles, n_database_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_score(index, index_amruta, similarity_matrix=similarity_matrix):\n",
    "    return {\"similarity_score\": similarity_matrix[index, index_amruta]}\n",
    "\n",
    "\n",
    "\n",
    "joined = files_df.with_row_index().join(database_df.with_row_index(), on=\"date\", suffix=\"_amruta\").with_columns(pl.struct(\"index\", \"index_amruta\").map_elements(lambda x: get_similarity_score(**x), return_dtype=pl.Struct({\"similarity_score\": pl.Float64, \"similarity_score_summary\": pl.Float64})).alias(\"similarity_score\"),\n",
    "                                                                                                                pl.col(\"file_stem\").str.contains(pl.col(\"City\")).alias(\"city_match\"),\n",
    "                                                                                                                pl.col(\"Title\").str.replace_all(\"[^A-Za-z]\", \"\").str.contains(pl.col(\"Title_amruta\").str.replace_all(\"[^A-Za-z]\", \"\").str.slice(0,5)).alias(\"title_match\"),\n",
    ").sort([\"file_stem\", \"similarity_score\"], descending=True).unnest(\"similarity_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = joined.filter(pl.any_horizontal(pl.col(\"similarity_score\")>0.1, \n",
    "                                pl.col(\"city_match\"), pl.col(\"title_match\")\n",
    "                                )\n",
    "              ).with_columns(pl.col(\"similarity_score\").sort(descending=True).over(\"file_stem\").alias(\"best_match\")).drop(\"best_match\", \"index\", \"index_amruta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.sort(pl.col(\"file_stem\").len().over(\"file_stem\"), descending=True).drop(\"date\").write_excel(\"matches.xlsx\", autofit=True, table_style=\"Table Style Light 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_matches = pl.read_excel(\"_matches.xlsx\")\n",
    "manually_matched_1 = _matches.filter(pl.col(\"is_match\") == \"1\")\n",
    "# manually_matched_1.write_excel(\"matches_1.xlsx\", autofit=True, table_style=\"Table Style Light 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=matches.filter(pl.col(\"Amruta\").is_in(manually_matched_1[\"Amruta\"], ).not_(),pl.col(\"file_stem\").is_in(manually_matched_1[\"file_stem\"]).not_()).sort(pl.col(\"file_stem\").len().over(\"file_stem\"), pl.col(\"Date\").dt.date(),\"Title\", \"similarity_score\", descending=True).drop(\"date\")\n",
    "a.write_excel(\"to_be_matched_manually_2.xlsx\", autofit=True, table_style=\"Table Style Light 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_matched_2 = pl.read_excel(\"_to_be_matched_manually_2.xlsx\").filter(pl.col(\"is_match\") == \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_matched_by_llohann = pl.concat([manually_matched_1, manually_matched_2], how=\"diagonal\")\n",
    "manually_matched_by_llohann.write_excel(\"manually_matched_by_llohann.xlsx\", autofit=True, table_style=\"Table Style Light 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df.filter(pl.col(\"file_stem\").is_in(manually_matched_by_llohann[\"file_stem\"]).not_()).write_excel(\"files_to_be_matched_manually.xlsx\", autofit=True, table_style=\"Table Style Light 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_matched_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
